\documentclass[a4paper,10pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{color}

\title{CS532100 Numerical Optimization Homework 2}
\author{109062710 Bijon Setyawan Raya}
\date{Due Dec 10}
\begin{document}
\maketitle
\begin{enumerate}
    \item Consider the linear least square problem:$$min_{\vec{x}\in \mathbb{R}^2}||A\vec{x} - \vec{b}||^2 ,
    $$ 
        where $$A = \left[ \begin{array}{cc}4 & 8 \\ 2 & 4 \\ 1 & 2 \end{array} \right], 
        \vec{b} = \left(\begin{array}{c} 21/4 \\ 0 \\ 0 \end{array} \right)
    $$

\begin{enumerate}
	\item (10\%) Write its normal equation.
    {\color{blue} 
        \begin{align}
        A^T A \vec{x} &= A^T \vec{b} \\
        \begin{bmatrix}4 & 2 & 1 \\ 8 & 4 & 2\end{bmatrix} \begin{bmatrix}4 & 8 \\ 2 & 4 \\ 1 & 2\end{bmatrix} \begin{bmatrix}x_1 \\ x_2\end{bmatrix} &= \begin{bmatrix}4 & 2 & 1 \\ 8 & 4 & 2\end{bmatrix} \begin{bmatrix}21/4 \\ 0 \\ 0\end{bmatrix}
        \end{align}
    }

    \item (10\%) Express $\vec{b} = \vec{b}_1 + \vec{b}_2$ such that $\vec{b}_1$ is in the subspace spanned by $A$'s column vectors, and $\vec{b}_2$ is orthogonal to $A$'s column vectors.
    {\color{blue} 
        From matrix A, we know that its basis is its first column which is $\begin{bmatrix}4 & 2 & 1\end{bmatrix}^T$. 
        From its basis column, we can express $\vec{b}$ in $\vec{b}_1 + \vec{b}_2$.

        Let, 
        \begin{align}
            \vec{b}_1 = \begin{bmatrix}4s \\ 2s \\ s\end{bmatrix}, \vec{b}_2 = \begin{bmatrix}t_1 \\ t_2 \\ t_3\end{bmatrix}
        \end{align}

        Then we can express $\vec{b}$ such as 
        \begin{align}
            \vec{b} &= \vec{b}_1 + \vec{b}_2 \\
            \begin{bmatrix}21/4 \\ 0 \\ 0\end{bmatrix} &= \begin{bmatrix}4s + t_1 \\ 2s + t_2 \\ s + t_3\end{bmatrix}
        \end{align}

        Since $\vec{b}_1$ is a column space of A, and $\vec{b}_2$ is orthogonal to $\vec{b}_1$, then
        \begin{align}
            \begin{bmatrix}4s \\ 2s \\ s\end{bmatrix} \begin{bmatrix}t_1 & t_2 & t_3\end{bmatrix} &= 0 \\ 
            4s \cdot t_1 + 2s \cdot t_2 + s \cdot t_3 &= 0
        \end{align}

        We can add $4s \cdot t_1 + 2s \cdot t_2 + s \cdot t_3 = 0$ at the last index of our matrix equality. Then
        $$
        \begin{bmatrix} 4s + t_1 \\ 2s + t_2 \\ s + t_3 \\ 4s \cdot t_1 + 2s \cdot t_2 + s \cdot t_3\end{bmatrix} = \begin{bmatrix} 21/4 \\ 0 \\ 0 \\ 0 \end{bmatrix}
        $$

        Solving the equality above, we then have $s = 1$, $t_1 = 5/4$, $t_2 = -2$, and $t_3 = -1$. Thus, 
        \begin{align}
            \vec{b}_1 = \begin{bmatrix}4s \\ 2s \\ s\end{bmatrix}, \vec{b}_2 = \begin{bmatrix}5/4 \\ -2 \\ -1\end{bmatrix} \\ 
            \begin{bmatrix}21/4 \\ 0 \\ 0\end{bmatrix} = \begin{bmatrix}4 \\ 2 \\ 1\end{bmatrix} + \begin{bmatrix}5/4 \\ -2 \\ -1\end{bmatrix}
        \end{align}
    }

    \item (10\%) Show that $\vec{z}\in \mathbb{R}^2$ is a least square solution for $A\vec{x}=\vec{b}$ if and only if $\vec{z}$ is part of a solution to the larger linear system:
    $$\left[ \begin{array}{cc}0 & A^T \\ A & I\end{array} \right] \left[\begin{array}{cc} \vec{z} \\ \vec{y} \end{array}\right] = \left[\begin{array}{cc} 0 \\ \vec{b}\end{array}\right]$$
    {\color{blue}
    }


\end{enumerate}
    \item In Note05 (Page 16), memoryless BFGS iteration matrix $H_{k+1}$ can be derived from considering the Hestenes–Stiefel form of the nonlinear conjugate gradient method. Recalling that $\vec{s}_k = \alpha_k \vec{p}_k$, we have that the search direction for this method
is given by
$$\vec{p}_{k+1} = -\nabla f_{k+1} + \frac{\nabla f_{k+1}^T\vec{y}_k}{\vec{y}^T\vec{p}_k}\vec{p}_k $$ $$ = -\nabla f_{k+1} + \frac{\nabla f_{k+1}^T\vec{y}_k}{\vec{y}^T\vec{s}_k}\vec{s}_k $$ $$ = -( I - \frac{\vec{s}_k\vec{y}_k^T}{\vec{y}^T\vec{s}_k})\nabla f_{k+1} $$ $$ = - \hat{H}_{k+1} \nabla f_{k+1}$$
However, the matrix $\hat{H}_{k+1}$ is neither symmetric nor positive deﬁnite.
\begin{enumerate}
    \item (10\%) Please show that the matrix $\hat{H}_{k+1}$ is singular. $\\$ (You can only prove it for the case $\nabla f_k, \vec{p}_k, \vec{y}_k, \vec{s}_k \in \mathbb{R}^2$ for all $k \in \mathbb{N}$.)
{\color{blue} asd}
    \item (0\%) Please read the reference book (Page 180) to understand the derivation of the inverse hessian formula in Note05 (Page 16). $\\$(you don't need to write anything in this subproblem.) $$H_{k+1} = (I - \frac{\vec{s}_k\vec{y}_k^T}{\vec{y}_k^T\vec{s}_k})(I - \frac{\vec{y}_k\vec{s}_k^T}{\vec{y}_k^T\vec{s}_k}) + \frac{\vec{s}_k\vec{s}_k^T}{\vec{y}_k^T\vec{s}_k}$$
\end{enumerate}

\item (10\%) The total least square problem is to solve the following problem
$$\min_{\vec{x}, \|\vec{x}\|=1} \vec{x}^TA^TA\vec{x}$$
where $A$ is an $m\times n$ matrix.  Here we assume $m>n$.  
Let $A=U\Sigma V^T$ be the SVD of $A$, where $U$ is the matrix of left singular vectors, $V$ is the matrix of right singular vectors, and $\Sigma$ is a diagonal matrix with diagonal elements
$\sigma_1, \sigma_2, \ldots, \sigma_n$.  Moreover, $U$ and $V$ are orthogonal matrices, and $\sigma_1\ge \sigma_2 \ge \cdots \ge \sigma_n$.
Show the solution of the above problem is the $\sigma^2$.

{\color{blue}
}

\item Consider the following linear programming problem:
$$\begin{array}{lll}
    \max_{x_1,x_2} & z=x_1+x_2 \\
    \mbox{s.t.} & x_1 + 2x_2 \le 4  \\
        & 4x_1 + 2x_2 \le 12   \\
        & -x_1 + x_2 \le 1   \\
        & x_1, x_2 \ge 0
    \end{array}$$

\begin{enumerate}
    \item (10\%) Please refer Note08 (Page 2) to draw the figure of the constraints by any means, and use that to solve the problem. 
    {\color{blue} 
    }

    \item (10\%) Derive its dual problem and solve the dual problem by any means. Compare the solutions of the primal and the dual problems.
    {\color{blue} 
    }

    \item (10\%) Verify the complementarity slackness condition.
    {\color{blue}
    }

    \item (10\%) Transform the problem to the standard form.
    {\color{blue} 
    }

    \item (10\%) Solve it by the simplex method, as provided in Figure 1, using $\vec{x}_0 = (0, 0)$.
    Indicate $B_k, N_k, \vec{s}_k, \vec{d}_k, p_k, q_k, \gamma_k$ in each step.\
    {\color{blue} 
    }
\end{enumerate}

\begin{figure}[hb]
    \begin{center}
    \begin{tabular}{cp{.2in}l}
        \hline \hline \\[0pt]
    (1) & \multicolumn{2}{l}{Given a basic feasible point $\vec{x}_0$ and the corresponding index set}\\
    & \multicolumn{2}{l}{ $\mathcal{B}_0$ and $\mathcal{N}_0$.} \\
    (2) & \multicolumn{2}{l}{For $k=0,1,\ldots$} \\
    (3) & & Let $B_k=A(:,\mathcal{B}_k), N_k=A(:,\mathcal{N}_k)$, $\vec{x}_B=\vec{x}_k(\mathcal{B}_k),
    \vec{x}_N=\vec{x}_k(\mathcal{N}_k)$, \\
        &  & and $\vec{c}_B=\vec{c}_k(\mathcal{B}_k), \vec{c}_N=\vec{c}_k(\mathcal{N}_k)$.\\
    (4) & & Compute $\vec{s}_k=\vec{c}_N-N_k^T(B_k^{-1})^T\vec{c}_B$  \mbox{\color{blue}(pricing)} \\
    (5) & & If $\vec{s}_k\ge 0$, return the solution $\vec{x}_k$. \mbox{\color{blue}(found optimal solution)} \\
    (6) & & Select $q_k\in\mathcal{N}_k$ such that $\vec{s}_k(i_q)<0$, \\
        & & where $i_q$ is the index of $q_k$ in $\mathcal{N}_k$\\
    (7) & & Compute $\vec{d}_k=B^{-1}_kA_k(:,q_k)$. \mbox{\color{blue}(search direction)} \\
    (8) & & If $\vec{d}_k\le 0$, return \verb"unbounded". \mbox{\color{blue}(unbounded case)} \\
    (9) & & Compute $\displaystyle [\gamma_k, i_p] = \min_{i,\vec{d}_k(i)>0}\frac{\vec{x}_B(i)}{\vec{d}_k(i)}$ \mbox{\color{blue}(ratio test)}\\
    &&\mbox{\color{blue}(The first return value is the minimum ratio;}\\
    &&\mbox{\color{blue} the second return value
    is the index of the minimum ratio.)} \\
    (10) & & $\displaystyle x_{k+1}\left(
                        \begin{array}{c}
                        \mathcal{B} \\
                        \mathcal{N} \\
                        \end{array}
                    \right) =  \left(
                        \begin{array}{c}
                        \vec{x}_B \\
                        \vec{x}_N \\
                        \end{array}
                    \right) + \gamma_k \left(
                        \begin{array}{c}
                        -\vec{d}_k \\
                        \vec{e}_{i_q} \\
                        \end{array}
                    \right)$ \\
    &&\mbox{\color{blue}($\vec{e}_{i_q}=(0,\ldots,1,\ldots,0)^T$ is a unit vector with $i_q$th element 1.)}\\
    (11) &&Let the $i_p$th element in $\mathcal{B}$ be $p_k$. \mbox{\color{blue}(pivoting)} \\
        & & $\mathcal{B}_{k+1}=(\mathcal{B}_k-\{p_k\})\cup\{q_k\}$,  $\mathcal{N}_{k+1}=(\mathcal{N}_k-\{q_k\})\cup\{p_k\}$  \\[10pt]
    \hline \hline
    \end{tabular}
    \end{center}
    \caption{The simplex method for solving (minimization) linear programming}\label{}
\end{figure}

\end{enumerate}

\end{document}