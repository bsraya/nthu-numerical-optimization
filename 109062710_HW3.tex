\documentclass[a4paper,10pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{color}

\title{CS532100 Numerical Optimization Homework 3}
\author{109062710}
\date{Due Jan 12}
\begin{document}
\maketitle
\begin{enumerate}
    \item (25\%)
    Consider the problem
    \begin{equation}\label{(1)}
        \begin{array}{cc}
            \displaystyle\min_{x_1,x_2} & 0.1\times(x_1-3)^2+x_2^2 \\
            \mbox{s.t.} & x_1^2+x_2^2 -1 \le 0
        \end{array}
    \end{equation}
    
    \begin{enumerate}
        \item Write down the KKT conditions for (1).

        {\color{blue}
            Let's define the Lagrangian Form first. The Langrangian form is
            \[
                L(x_1, x_2, \lambda) = 0.1(x_1 - 3)^2 + x_2^2 - \lambda (1 - x_1^2 - x_2^2)
            \]

            The KKT conditions are
            \begin{enumerate}
                \item $\nabla_{x_1, x_2} L(x_1^*, x_2^*, \lambda^*) = 0$
                \item $(x_1^*)^2, (x_2^*)^2$
                \item $\lambda^* \geq 0 $
                \item $\lambda^*(1 - (x_1^*)^2 - (x_2^*)^2)$
            \end{enumerate}
        }

        \item Solve the KKT conditions and find the optimal solutions, including the Lagrangian parameters.
        {\color{blue}
            \[ 
                \nabla L = \begin{bmatrix}0.2(x_1 - 3) + 2 \lambda x_1 \\ 2x_2 + 2 \lambda x_2\end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
            \]
            From one of the conditions above, it violates the constraints if $\lambda = 0$, $x_1 = 3$, and $x_2 = 0$.
            So when $\lambda > 0$  and $x_1^2 + x_2^2 = 1$. 
            The second equation in $\nabla L = 0$, which is $(1+\lambda)x_2 = 0$, indicates either $x_2 = 0$ or $\lambda = -1$.
            Since $\lambda > 0$ and $x_2^* = 0$, therefore $x_1^* = 1$.
            Calculating the $\lambda^*$ the first equation in $\nabla L = 0$, thus $0.2 \cdot 2 (1-3) + 2 \lambda = 0$. 
        }

        \item Compute the reduced Hessian and check the second order conditions for the solution.
        {\color{blue}
            The Hessian of the Langrange is
            \[
                \nabla^2 L = \begin{bmatrix} 0.2 + 2\lambda & 0 \\ 0 & 2 + 2\lambda \end{bmatrix} = \begin{bmatrix} 0.6 & 0 \\ 0 & 2.4 \end{bmatrix}
            \]
            
            Let $A = (\nabla c)^*$ is at the optimal solution.
            \[
                A = \begin{bmatrix} \frac{\partial c}{\partial x_1} \frac{\partial c}{\partial x_2} \end{bmatrix} = \begin{bmatrix} 2 & 0 \end{bmatrix}
            \]

            The null space is $\omega = \begin{bmatrix} 0 & \omega_2 \end{bmatrix}^T$.

            The reduced Hessian is
            \begin{align}
                \omega^T \nabla^2 L \omega &= 
                \begin{bmatrix} 0 & \omega_2 \end{bmatrix} 
                \begin{bmatrix} 0.6 & 0 \\ 0 & 2.4 \end{bmatrix} 
                \begin{bmatrix} 0 \\ \omega_2 \end{bmatrix} \\
                &= \begin{bmatrix} 0 & 2.4 \cdot \omega_2 \end{bmatrix} \begin{bmatrix} 0 \\ \omega_2 \end{bmatrix} \\
                &= 2.4 \cdot \omega_2^2 > 0
            \end{align}
        }
    \end{enumerate}


    \item (20\%) Consider the problem 
    \begin{equation}\label{(2)}
        \begin{array}{cc}
            \displaystyle\min_{x_1,x_2, x_3} & (x_1-1)^2+(x_2 - 1)^2 + (x_3 - 1)^2 \\
            \mbox{s.t.} & 0\le x_1, x_2, x_3\le 2.
        \end{array}
    \end{equation}
    Find the optimization solution $x^{\ast}$ for (2) with gradient projection method, with initial guess at $\vec{x}_0 =  (x_1, x_2, x_3) = (0, 0, 2)^T.\\$
    (Find out all segments and the minimizers of all segments, and determine whether the solution you got from this method is optimal solution. Justify your answer.)
    {\color{blue}
        The gradient at $\vec{x}_0 = (x_1, x_2, x_3) = (0 0 2)^T$ is 
        \[
            g = \begin{bmatrix} 2(x_1 - 1) \\ 2(x_2 - 1) \\ 2(x_3 - 1) \end{bmatrix} = \begin{bmatrix} -2 & -2 & 2 \end{bmatrix}
        \]

        \begin{align}
            t_1 &= -2 / -2 = 1 \\ 
            t_2 &= -2 / -2 = 1 \\ 
            t_3 &= -2 / 2 = -1
        \end{align}

        We then need to process the segments sequentially such that $(0, t_3) \rightarrow (t_3, t_2) \rightarrow (t_2, t_1)$.
        \[
            \vec{x}(t) = \begin{bmatrix} 0 \\ 0 \\ 2 \end{bmatrix} \rightarrow \begin{bmatrix} 2t\\ 2t \\ 2t \end{bmatrix} \rightarrow \begin{bmatrix} -2 \\ 2t \\ 2t \end{bmatrix}_{t = -1} \rightarrow \begin{bmatrix} -2 \\ 2 \\ 2t \end{bmatrix}_{t = 1} \rightarrow \begin{bmatrix} -2 \\ 2 \\ 2 \end{bmatrix}_{t = 1}
        \]

        In the first segment, we have 
        \[
            \min_{t} (2t - 1)^2 + (2t-1)^2 + (2t-1)^2 = \min_{t} 3(2t-1)^2
        \]

        The optimal solution is at $t = \frac{1}{2}$, which is larger than $t = -1$, so we set $t = -1$. With $t = -1$, then $f(x(t)) = 27$.

        In the second segment 
        \[
            \min_{t} (-2 - 1)^2 + (2t-1)^2 + (2t-1)^2 = \min_{t} 2(2t-1)^2 + 9
        \]

        Its optimal solution is at $t = \frac{1}{2}$, then $f(x(t)) = 9$.

        Thus, the optimal solution is at $(-2, 2, 2)$. Plugging $(-2, 2, 2)$ to the original formula, we then have
        \[
            (-2-1)^2 + (2-1)^2 + (2-1)^2 = 9 + 1 + 1 = 11
        \]
    }


\item (15\%) Consider the problem 
\begin{equation}\label{(3)}
    \begin{array}{cc}
        \displaystyle\min_{x_1,x_2} & z = 8x_1 + 5x_2 \\
        \mbox{s.t.} & 2x_1 + x_2 \le 100  \\
        & 3x_1 + 4x_2 \le 240   \\
        & x_1, x_2 \ge 0
    \end{array}
\end{equation}
Formulate this problem to the equation of the interior point method, and
derive the gradient of the Lagrangian and the Jacobian of the function F.
(The "gradient" means F and the Jacobian is the derivative of F.)

{\color{blue} 
First, the primal and the dual problems should be defined.

The primal problem is
\[
    \begin{array}{cc}
        \displaystyle\min_{x_1,x_2} & 8x_1 + 5x_2 \\
        \mbox{s.t.} & 2x_1 + x_2 + x_3 = 100  \\
        & 3x_1 + 4x_2 + x_4 = 240   \\
        & x_1, x_2, x_3, x_4 \ge 0
    \end{array}
\]

The dual problem is
\[
    \begin{array}{cc}
        \displaystyle \max_{y_1,y_2} & 100x_1 + 240x_2 \\
        \mbox{s.t.} & 2 y_1 + 3 y_2 + s_1 = 8  \\
        & y_1 + 4 y_2= 5 \\
        & s_1, s_2, s_3, s_4 \ge 0
    \end{array}
\]

\[
    A = \begin{bmatrix}
        2 & 1 & 1 & 0 \\ 3 & 4 & 0 & 1
    \end{bmatrix},
    C = \begin{bmatrix} 8 \\ 5 \\ 0 \\ 0 \end{bmatrix}, 
    X = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix}
    s = \begin{bmatrix} s_1 \\ s_2 \\ s_3 \\ s_4 \end{bmatrix}
    b = \begin{bmatrix} 100 \\ 240 \end{bmatrix}
\]

The gradient is
\[
    F = \begin{bmatrix} A^T y + s -c \\ Ax - b \\ X \cdot s \end{bmatrix} = \begin{bmatrix} 2y_1 + 3y_2 + s_1 - 8 \\ y_1 + 4y_2 + s_2 - 5 \\ y_1 + s_3 \\ y_2 + s_4 \\ 2x_1 + x_2 + x_3 - 100 \\ 3x_1 + 4x_2 + x_4 - 240 \\ x_1 s_1 \\ x_2 s_2 \\ x_3 s_3 \\ x_3 s_4 \end{bmatrix} = 0
\]

The Jacobian matrix is
\begin{align}
    J &= \begin{bmatrix} \nabla_x r_c & \nabla_y r_c & \nabla_s r_c \\ \nabla_x r_b & \nabla_y r_b & \nabla_s r_b \\ \nabla_x r_s & \nabla_y r_s & \nabla_s r_s \end{bmatrix} \\
    &= \begin{bmatrix} 0 & A^T & I \\ A & 0 & 0 \\ S & 0 & X \end{bmatrix} \\ 
    &= \begin{bmatrix} 
        0 & 0 & 0 & 0 & 2 & 3 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 4 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\ 
        0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 \\
        2 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
        3 & 4 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
        s_1 & 0 & 0 & 0 & 0 & 0 & x_1 & 0 & 0 & 0 \\
        0 & s_2 & 0 & 0 & 0 & 0 & 0 & x_2 & 0 & 0 \\
        0 & 0 & s_3 & 0 & 0 & 0 & 0 & 0 & x_3 & 0 \\
        0 & 0 & 0 & s_4 & 0 & 0 & 0 & 0 & 0 & x_4
    \end{bmatrix}
\end{align}
}

\item (20\%) Consider the following constrained optimization problem
    \begin{equation}\label{(4)}
        \begin{array}{cc}
            \displaystyle\min_{x_1,x_2} & x_1 + x_2 \\
            \mbox{s.t.} & x_1^2 + x_2^2 = 2 \\
        \end{array}
    \end{equation}

    \begin{enumerate}
        \item Write the augmented Lagrangian penalty function $L$ and Hessian of $L$ of this problem. 
        
        {\color{blue} 
            The augmented Lagrangian penalty function is
            \[
                x_1 + x_2 - \rho (x_1^2 + x_2^2 - 2) + \frac{\mu}{2}(x_1^2 + x_2^2 - 2)^2
            \]

            The gradient of $L$ is
            \[
                \nabla L(x_1, x_2, \rho) = \begin{bmatrix}
                    1 - 2 \rho x_1 + 2 \mu x_1 (x_1^2 + x_2^2 - 2) \\
                    1 - 2 \rho x_2 + 2 \mu x_2 (x_1^2 + x_2^2 - 2)
                \end{bmatrix}
            \]

            The Hessian of $L$ is
            \[
                \nabla^2 L(x_1, x_2, \rho) = \begin{bmatrix}
                    -2 \rho + 2 \mu (3x_1^2 + x_2^2 - 2) & 4 \mu x_1 x_2 \\ 
                    4 \mu x_1 x_2 & -2 \rho + 2 \mu (x_1^2 + 3x_2^2 - 2)
                \end{bmatrix}
            \]
        }

        \item To make the augmented Lagrangian function $L$ exact, what is the penalty parameter $\mu$ should be ?
        {\color{blue} 
            First we should find the value of $x_1$, $x_2$, and $\lambda$ which will result the smallest value. 

            Let's define the Langrangian form first.
            \[
                \nabla L(x_1, x_2, \lambda) = \begin{bmatrix} 1 - 2 \lambda x_1 \\ 1 - 2 \lambda x_2 \\ x_1^2 + x_2^2 - 2 \end{bmatrix} = 0 
            \]
            
            Solving the equations in the matrix, we then have $\lambda = 1/2$ and $\lambda = -1/2$.
            When $\lambda = 1/2$, the $x_1 = 1$ and $x_2 = 1$.
            When $\lambda = -1/2$, the $x_1 = -1$ and $x_2 = -1$.
            Since $\lambda = -1/2$, $x_1 = -1$ and $x_2 = -1$ configurations give us the lowest value if those values are plugged in $x_1 + x_2$, we then use these values to find the minimum $\mu$.

            At $x_1 = -1$ and $x_2 = -1$, the Hessian matrix becomes
            \[
                \begin{bmatrix}
                    4\mu & 4\mu \\
                    4\mu & 4\mu
                \end{bmatrix}
            \]

            To make sure all the values in the Hessian matrix above are positive, $\mu > 0$.
        }
    \end{enumerate}

\item (5\%) Find the condition number $\kappa(A)$ of matrix A. Describe how ill-
conditioned and good-conditioned matrices behave in matrix computation.
\[
    A = \begin{bmatrix} 4 & 0 & 0 \\0 & 0 & 2 \\0 & 3 & 0 \end{bmatrix}
\]

{\color{blue} 
In order to calculate the condition number of matrix $A$, we need to know that $cond(A) = ||A|| \cdot ||A^{-1}||$.

However, we should know that $||A||$ the n-norm of matrix $A$. Since the norm-2 is the most common one, we are going to use $||A||_2$ and $||A^{-1}||_2$ 

The inverse of matrix $A$ is

\[
    A^{-1} = \begin{bmatrix}
        1/4 & 0 & 0 \\ 0 & 0 & 1/3 \\ 0 & 1/2 & 0
    \end{bmatrix}
\]

Calculating both $||A||_2$ and $||A||^{-1}_2$, we get $||A||_2 = 4$ and $||A^{-1}||_2 = 0.5$.
Thus, $cond_2(A) = ||A||_2 \cdot ||A^{-1}||_2 = 2$.

Other results:
\begin{enumerate}
    \item Norm-1 $\rightarrow cond_1(A) = 2$
    \item Norm-$\infty$ $\rightarrow cond_\infty(A) = 2$
    \item Frobenius norm $\rightarrow cond_{fro}(A) = 3.505$
    \item Max norm $\rightarrow cond_{\max}(A) = 2$   
\end{enumerate}

Ill-conditioned number is a condition where a small change in the input value will result in a large change in the output value.

As oppose to ill-conditioned number, well-conditioned number is a condition where a little or large change to the input in a function will result in a little change to the output.

Therefore, a condition number of $0$ would mean that the matrix removes any error, which isn't possible. However

In fact, the smallest possible condition number is $1$, where an error is neither magnified nor diminished.
}

\item (15\%) The problem 15.4 in textbook shows an example of Maratos effect.
		\begin{align*}
		\min_{x_1, x_2} & f( x_1, x_2 ) = 2( x_1^2 + x_2^2 -1) -x_1\\
		\mbox{s.t. } & x_1^2 + x_2^2 -1=0
		\end{align*}
	The optimal solution is $\vec{x}^* = (1,0)$. Suppose 
	$\vec{x}_k =
		\left(
		\begin{array}{l}
			\cos \theta\\
			\sin \theta
		\end{array} \right),
		\vec{p}_k =
		\left(\begin{array}{c}
			\sin^2 \theta\\
			- \sin \theta \cos \theta
		\end{array} \right)$
	The Maratos effect says although $\vec{p}_k$ is a good step, but the filter method will reject it. In the textbook and slides, 
	it says the remedy to this problem is using the second order correction. Read the textbook or the slides to understand the reason why the second order correction can help avoiding this problem. And explain it in your own words. 
    
    {\color{blue}
        Suppose 
        \begin{align}
            \vec{x}_k &= \begin{bmatrix} cos \theta \\ sin \theta \end{bmatrix} \\
            \vec{p}_k &= \begin{bmatrix} sin^2 \theta \\ - sin \theta cos \theta \end{bmatrix}\\
            \vec{x}_{k+1} &= \vec{x}_k + \vec{p}_k \\
            &= \begin{bmatrix} cos \theta + sin^2 \theta \\ sin \theta (1 - cos \theta) \end{bmatrix}
        \end{align}

        We then calculate $||\vec{x}_k - \vec{x}^*||$ and $||\vec{x}_{k+1} - \vec{x}^*||$.
        \begin{align}
            ||\vec{x}_k - \vec{x}^*|| &= \begin{bmatrix} \cos \theta-1 \\ \sin \theta \end{bmatrix} \\
            &=\sqrt{\cos^{2} \theta-2\cos\theta+ 1 +\sin^{2} \theta} \\
            &=\sqrt{2(1-\cos\theta)}
        \end{align}

        \begin{align}
            ||\vec{x}_{k+1}-\vec{x}^{*}|| &= \begin{bmatrix}
            \cos \theta+\sin ^{2} \theta-1 \\
            \sin \theta-\sin \theta \cos \theta
            \end{bmatrix} \\
            &= \begin{bmatrix}
            \cos \theta(1-\cos \theta) \\
            \sin \theta(1-\cos \theta)
            \end{bmatrix} \\
            &=\sqrt{\cos ^{2} \theta(1-\cos \theta)^{2}+\sin ^{2} \theta(1-\cos \theta)^{2}} \\
            &=\sqrt{(1-\cos \theta)^{2}}
        \end{align}

        Thus,
        \[
            \frac{||\vec{x}_{k+1} - \vec{x}^*||}{||\vec{x}_k - \vec{x}^*||} = \frac{1}{2}
        \]

        Unfortunately, the filter method is going to reject this step because 

        \begin{align}
            f(\vec{x}_k) &= - \cos \theta \text{ , and } c(\vec{x}_k) = 0 \\
            f(\vec{x}_{k+1}) &= - \cos \theta - \sin 2 \theta = \sin^2 \theta - \cos \theta > f(\vec{x}_k) \\
            c(\vec{x}_{k+1}) &= \sin^2 \theta > 0 = (\vec{x}_k)
        \end{align}

        This is what is called the Maratos Effect.

        The second order correction is believed to be able to solve The Maratos Effect. Instead of stating that $\nabla c\left(\vec{x}_{k}\right)^{T} \vec{p}_{k}+c\left(\vec{x}_{k}\right)=0$, let's express it in a quadratic approximation form.

        \[
            c \left(\vec{x}_{k}\right)+\nabla c\left(\vec{x}_{k}\right)^{T} \vec{d}_{k}+\frac{1}{2} \vec{d}_{k}^{T} \nabla_{x x}^{2} c(\vec{x}) \vec{d}_{k}=0
        \]

        Suppose $||\vec{d}_{k}-\vec{p}_{k}||$ is very small. Let's use Taylor expansion in order to approximate the quadratic term.

    
        \begin{align}
            c\left(\vec{x}_{k}+\vec{p}_{k}\right) &\approx c\left(\vec{x}_{k}\right)+\nabla c\left(\vec{x}_{k}\right)^{T} \vec{p}_{k}+\frac{1}{2} \vec{p}_{k}^{T} \nabla_{x x}^{2} c(\vec{x}) \vec{p}_{k} \\
            \frac{1}{2} \vec{d}_{k}^{T} \nabla_{x x}^{2} c(\vec{x}) \vec{d}_{k} &\approx \frac{1}{2} \vec{p}_{k}^{T} \nabla_{x x}^{2} c(\vec{x}) \vec{p}_{k} \\ 
            &\approx c\left(\vec{x}_{k}+\vec{p}_{k}\right)-c\left(\vec{x}_{k}\right)-\nabla c\left(\vec{x}_{k}\right)^{T} \vec{p}_{k}
        \end{align}

        Using the $(30)$ can be simplified to
        \[
            \nabla c\left(\vec{x}_{k}\right)^{T} \vec{d}_{k}+c\left(\vec{x}_{k}+\vec{p}_{k}\right)-\nabla c\left(\vec{x}_{k}\right)^{T} \vec{p}_{k}=0
        \]

        Express the equation above with the corrected linearized constraint
        \[
            \nabla c\left(\vec{x}_{k}\right)^{T} \vec{p}+c\left(\vec{x}_{k}+\vec{p}_{k}\right)-\nabla c\left(\vec{x}_{k}\right)^{T} \vec{p}_{k}=0
        \]
    }

\end{enumerate}

\end{document}