{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4621f47",
   "metadata": {},
   "source": [
    "# Numerical Optimization (CS215300) Assignment 3\n",
    "## Introduction\n",
    "In this assignment, we expect you to be able to solve constrained optimization problem by Scipy library. We want you to apply two algorithms on the following problem as benchmark, survey the method used in these libraries, and analysis the behaviour of these algorithms.\n",
    "The library document link: https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html\n",
    "\n",
    "## Task\n",
    "1. Please solve the following problrem by using `trust-constr` method:\n",
    "\n",
    "$$\n",
    "        \\begin{array}{cc}\n",
    "                \\displaystyle \\min_{x_1,x_2} & f(x_1,x_2)=-x_1-x_2 \\\\\n",
    "        \\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "        \\text{s.t.} -2x_1^4 + 8x_1^3 -8x_1^2 + x_2 - 2 \\le 0  \\\\\n",
    "        -4x_1^4 + 32x_1^3 - 88x_1^2 + 96x_1 + x_2 -36 \\le 0   \\\\\n",
    "        0 \\le x_1 \\le 3 \\\\\n",
    "        0 \\le x_2 \\le 4 \\\\\n",
    "$$\n",
    "\n",
    "2. Please use COBYLA method to solve the same problem.\n",
    "3. In your report, please read the paper cited in the libraries, which gives the details of the algorithms. Write a brief introduction of the algorithms, and compare their behaviours in this benchmark. You are not necessarily to read the original paper, other resourses (ex. slides from other schools or surveys) are also acceptable, please include the link or paper name in your reference if you referred other resources.\n",
    "4. Please provide the Jacobian and Hessian function in matrix form in your report. Basic latex syntax is supported in Markdown.\n",
    "5. Rename this notebook file with your student ID and upload it to eeclass platform. (ex. 107xxxxxx.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4332e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import NonlinearConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492c200",
   "metadata": {},
   "source": [
    "### Define objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9c5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return - x[0] - x[1]\n",
    "\n",
    "def grad(x):\n",
    "    return np.array([-1.0, -1.0])\n",
    "\n",
    "def hess(x):\n",
    "    return np.array([[0.0,0.0],[0.0,0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc339d70",
   "metadata": {},
   "source": [
    "### Define constraint functions and derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af6479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons_f(x):\n",
    "    constrain1 = (-2.0*x[0]**4) +  (8.0*x[0]**3) -  (8.0*x[0]**2) + x[1] - 2.0\n",
    "    constrain2 = (-4.0*x[0]**4) + (32.0*x[0]**3) - (88.0*x[0]**2) + (96.0*x[0]) + x[1] - 36.0\n",
    "    return [constrain1, constrain2]\n",
    "\n",
    "def cons_Jacobian(x):\n",
    "    jacobian1 = [(-8.0*(x[0]**3)) + (24.0*(x[0]**2)) - (16.0*x[0]), 1.0]\n",
    "    jacobian2 = [(-16.0*(x[0]**3)) + (96.0*(x[0]**2)) - (176.0*x[0]) + 96.0, 1.0]\n",
    "    return [jacobian1, jacobian2]\n",
    "\n",
    "def cons_Hessian(x, v):\n",
    "    hessian1 = np.array([[(-24.0*x[0]**2) +  (48.0*x[0]) -  16.0, 0.0], [0.0, 0.0]])\n",
    "    hessian2 = np.array([[(-48.0*x[0]**2) + (192.0*x[0]) - 176.0, 0.0], [0.0, 0.0]])\n",
    "    return v[0]*hessian1 + v[1]*hessian2\n",
    "\n",
    "nonlinear_constraint = NonlinearConstraint(\n",
    "    cons_f,  \n",
    "    -np.inf,\n",
    "    0,\n",
    "    jac = cons_Jacobian, \n",
    "    hess = cons_Hessian\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f1e0c",
   "metadata": {},
   "source": [
    "### Call the minimize library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4cc4e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 17, function evaluations: 13, CG iterations: 12, optimality: 7.68e-09, constraint violation: 0.00e+00, execution time: 0.037 s.\n",
      "[0.61157004 3.44188615]\n"
     ]
    }
   ],
   "source": [
    "bounds = Bounds([0, 0], [3.0, 4.0])\n",
    "x0 = [0,0]\n",
    "res = minimize(\n",
    "    f, \n",
    "    x0, \n",
    "    method = 'trust-constr', \n",
    "    jac = grad, \n",
    "    hess = hess, \n",
    "    constraints = [nonlinear_constraint], \n",
    "    options = {'verbose': 1},\n",
    "    bounds = bounds\n",
    ")\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f40dee",
   "metadata": {},
   "source": [
    "### Apply COBYLA method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d91b12c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61160344 3.44210482]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bijonsetyawan/.local/lib/python3.9/site-packages/scipy/optimize/_constraints.py:356: OptimizeWarning: Constraint options `finite_diff_jac_sparsity`, `finite_diff_rel_step`, `keep_feasible`, and `hess`are ignored by this method.\n",
      "  warn(\"Constraint options `finite_diff_jac_sparsity`, \"\n"
     ]
    }
   ],
   "source": [
    "x0 = [0,0]\n",
    "res = minimize(\n",
    "    f, \n",
    "    x0, \n",
    "    method = 'cobyla', \n",
    "    constraints = [nonlinear_constraint]\n",
    ")\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bf609",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "In this homework, the result that I got from `trust-constr` and `cobyla` are exactly similar. The result is `[0.61160344 3.44210482]`.\n",
    "\n",
    "## The Objective Function\n",
    "\n",
    "$$\n",
    "    f(x_1, x_2) = -x_1 - x_2\n",
    "$$\n",
    "\n",
    "## The Gradient Objective Function\n",
    "\n",
    "$$\n",
    "    \\nabla f(x_1, x_2) = \\begin{bmatrix} -1 & -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## The Hessian Objective Function\n",
    "\n",
    "$$\n",
    "    \\nabla^2 f(x_1, x_2) = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## The Constraints\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        -2x_1^4 + 8x_1^3 -8x_1^2 + x_2 - 2  \\\\\n",
    "        -4x_1^4 + 32x_1^3 - 88x_1^2 + 96x_1 + x_2 -36\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## The Constraint Jacobian Matrix\n",
    "\n",
    "$$\n",
    "    \\nabla J(x) = \\begin{bmatrix}\n",
    "        -8x_1^3 + 24x_1 - 16 x_1 && 1 \\\\\n",
    "        -16x_1^3 + 96x_1^2 - 176x_1 + 96 && 1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## The Constraint Hessian Matrix\n",
    "\n",
    "$$\n",
    "    \\nabla H(x, v) = v_0 \\begin{bmatrix}\n",
    "        -24x_1^2 + 8 & 0 \\\\ 0 & 0\n",
    "    \\end{bmatrix} + v_1 \\begin{bmatrix}\n",
    "        -48x_1^2 + 192x_1 - 176 & 0 \\\\ 0 & 0\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
