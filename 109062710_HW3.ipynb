{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4621f47",
   "metadata": {},
   "source": [
    "# Numerical Optimization (CS215300) Assignment 3\n",
    "## Introduction\n",
    "In this assignment, we expect you to be able to solve constrained optimization problem by Scipy library. We want you to apply two algorithms on the following problem as benchmark, survey the method used in these libraries, and analysis the behaviour of these algorithms.\n",
    "The library document link: https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html\n",
    "\n",
    "## Task\n",
    "1. Please solve the following problrem by using `trust-constr` method:\n",
    "\n",
    "$$\n",
    "        \\begin{array}{cc}\n",
    "                \\displaystyle \\min_{x_1,x_2} & f(x_1,x_2)=-x_1-x_2 \\\\\n",
    "        \\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "        \\text{s.t.} -2x_1^4 + 8x_1^3 -8x_1^2 + x_2 - 2 \\le 0  \\\\\n",
    "        -4x_1^4 + 32x_1^3 - 88x_1^2 + 96x_1 + x_2 -36 \\le 0   \\\\\n",
    "        0 \\le x_1 \\le 3 \\\\\n",
    "        0 \\le x_2 \\le 4 \\\\\n",
    "$$\n",
    "\n",
    "2. Please use COBYLA method to solve the same problem.\n",
    "3. In your report, please read the paper cited in the libraries, which gives the details of the algorithms. Write a brief introduction of the algorithms, and compare their behaviours in this benchmark. You are not necessarily to read the original paper, other resourses (ex. slides from other schools or surveys) are also acceptable, please include the link or paper name in your reference if you referred other resources.\n",
    "4. Please provide the Jacobian and Hessian function in matrix form in your report. Basic latex syntax is supported in Markdown.\n",
    "5. Rename this notebook file with your student ID and upload it to eeclass platform. (ex. 107xxxxxx.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4332e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import NonlinearConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492c200",
   "metadata": {},
   "source": [
    "### Define objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d9c5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return - x[0] - x[1]\n",
    "\n",
    "def grad(x):\n",
    "    return np.array([-1.0, -1.0])\n",
    "\n",
    "def hess(x):\n",
    "    return np.array([[0.0,0.0],[0.0,0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc339d70",
   "metadata": {},
   "source": [
    "### Define constraint functions and derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8af6479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons_f(x):\n",
    "    constrain1 = (-2.0*x[0]**4) +  (8.0*x[0]**3) -  (8.0*x[0]**2) + x[1] - 2.0\n",
    "    constrain2 = (-4.0*x[0]**4) + (32.0*x[0]**3) - (88.0*x[0]**2) + (96.0*x[0]) + x[1] - 36.0\n",
    "    return [constrain1, constrain2]\n",
    "\n",
    "def cons_Jacobian(x):\n",
    "    jacobian1 = [(-8.0*(x[0]**3)) + (24.0*(x[0]**2)) - (16.0*x[0]), 1.0]\n",
    "    jacobian2 = [(-16.0*(x[0]**3)) + (96.0*(x[0]**2)) - (176.0*x[0]) + 96.0, 1.0]\n",
    "    return [jacobian1, jacobian2]\n",
    "\n",
    "def cons_Hessian(x, v):\n",
    "    hessian1 = np.array([[(-24.0*x[0]**2) +  (48.0*x[0]) -  16.0, 0.0], [0.0, 0.0]])\n",
    "    hessian2 = np.array([[(-48.0*x[0]**2) + (192.0*x[0]) - 176.0, 0.0], [0.0, 0.0]])\n",
    "    return v[0]*hessian1 + v[1]*hessian2\n",
    "\n",
    "nonlinear_constraint = NonlinearConstraint(\n",
    "    cons_f,  \n",
    "    -np.inf,\n",
    "    0,\n",
    "    jac = cons_Jacobian, \n",
    "    hess = cons_Hessian\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f1e0c",
   "metadata": {},
   "source": [
    "### Call the minimize library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4cc4e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 17, function evaluations: 13, CG iterations: 12, optimality: 7.68e-09, constraint violation: 0.00e+00, execution time: 0.031 s.\n",
      "[0.61157004 3.44188615]\n"
     ]
    }
   ],
   "source": [
    "bounds = Bounds([0, 0], [3.0, 4.0])\n",
    "x0 = [0,0]\n",
    "res = minimize(\n",
    "    f, \n",
    "    x0, \n",
    "    method = 'trust-constr', \n",
    "    jac = grad, \n",
    "    hess = hess, \n",
    "    constraints = [nonlinear_constraint], \n",
    "    options = {'verbose': 1},\n",
    "    bounds = bounds\n",
    ")\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f40dee",
   "metadata": {},
   "source": [
    "### Apply COBYLA method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd943a6",
   "metadata": {},
   "source": [
    "Since 'COBYLA' requires constraints to be defined as a sequence of dictionaries, with `type`, `fun` and `jac`, we have to redefined our inequality contraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91b12c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.34481012 208.81549884]\n",
      "   Return from subroutine COBYLA because rounding errors are becoming damaging.\n",
      "\n",
      "   NFVALS =  417   F =-2.131603E+02    MAXCV = 7.651946E-01\n",
      "   X = 4.344810E+00   2.088155E+02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ineq_cons = {\n",
    "    'type': 'ineq',\n",
    "    'fun' : lambda x: np.array(\n",
    "        (-2.0*x[0]**4) +  (8.0*x[0]**3) -  (8.0*x[0]**2) + x[1] - 2.0,  \n",
    "        (-4.0*x[0]**4) + (32.0*x[0]**3) - (88.0*x[0]**2) + (96.0*x[0]) + x[1] - 36.0\n",
    "    ),\n",
    "    'jac' : lambda x: np.array(\n",
    "        (-8.0*(x[0]**3)) + (24.0*(x[0]**2)) - (16.0*x[0]), 1.0,\n",
    "        (-16.0*(x[0]**3)) + (96.0*(x[0]**2)) - (176.0*x[0]) + 96.0, 1.0\n",
    "    )\n",
    "}\n",
    "\n",
    "x0 = [0,0]\n",
    "res = minimize(\n",
    "    f, \n",
    "    x0, \n",
    "    method = 'cobyla', \n",
    "    constraints = [ineq_cons],\n",
    "    options={'disp': True}\n",
    ")\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bf609",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "### 1. Trust Region Method\n",
    "\n",
    "The result that I got from `trust-constr` is `[0.61157004 3.44188615]` where the initial guess is `[0,0]`.\n",
    "\n",
    "Before discussing how Trust Region Method, or TRM, works, first we have to define what is a trust region. Trust region is a subset of region of an objective function that is acquired from a quadratic function. \n",
    "\n",
    "It works in a way that first define a region around the current best solution, in which a certain model (usually a quadratic model) can to some extent approximate the original objective function. TRM then take a step forward according to the model depicts within the region. Unlike the line search methods, TRM usually determines the step size before the improving direction (or at the same time). If a notable decrease (our following discussion will based on minimization problems) is gained after the step forward, then the model is believed to be a good representation of the original objective function. If the improvement is too subtle or even a negative improvement is gained, then the model is not to be believed as a good representation of the original objective function within that region. The convergence can be ensured that the size of the “trust region” (usually defined by the radius in Euclidean norm) in each iteration would depend on the improvement previously made. \n",
    "\n",
    "### 2. COBYLA Method\n",
    "\n",
    "The result that I got from `cobyla` is `[4.34481012 208.81549884]`.\n",
    "\n",
    "Unlike `trust-constr`, Constrained optimization by linear approximation, or COBYLA, is a numerical optimization algorithm for constrained problems where the derivative of the objective function is not known.\n",
    "\n",
    "It works by iteratively approximating the actual constrained optimization problem with linear programming problems. During an iteration, an approximating linear programming problem is solved to obtain a candidate for the optimal solution. The candidate solution is evaluated using the original objective and constraint functions, yielding a new data point in the optimization space. This information is used to improve the approximating linear programming problem used for the next iteration of the algorithm. When the solution cannot be improved anymore, the step size is reduced, refining the search. When the step size becomes sufficiently small, the algorithm finishes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af73b0",
   "metadata": {},
   "source": [
    "# Functions & Matrices Needed\n",
    "\n",
    "### The Objective Function\n",
    "\n",
    "$$\n",
    "    f(x_1, x_2) = -x_1 - x_2\n",
    "$$\n",
    "\n",
    "### The Gradient Objective Function\n",
    "\n",
    "$$\n",
    "    \\nabla f(x_1, x_2) = \\begin{bmatrix} -1 & -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### The Hessian Objective Function\n",
    "\n",
    "$$\n",
    "    \\nabla^2 f(x_1, x_2) = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### The Constraints\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        -2x_1^4 + 8x_1^3 -8x_1^2 + x_2 - 2  \\\\\n",
    "        -4x_1^4 + 32x_1^3 - 88x_1^2 + 96x_1 + x_2 -36\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### The Constraint Jacobian Matrix\n",
    "\n",
    "$$\n",
    "    \\nabla J(x) = \\begin{bmatrix}\n",
    "        -8x_1^3 + 24x_1 - 16 x_1 && 1 \\\\\n",
    "        -16x_1^3 + 96x_1^2 - 176x_1 + 96 && 1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### The Constraint Hessian Matrix\n",
    "\n",
    "$$\n",
    "    \\nabla H(x, v) = v_0 \\begin{bmatrix}\n",
    "        -24x_1^2 + 8 & 0 \\\\ 0 & 0\n",
    "    \\end{bmatrix} + v_1 \\begin{bmatrix}\n",
    "        -48x_1^2 + 192x_1 - 176 & 0 \\\\ 0 & 0\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ce0be",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [https://optimization.mccormick.northwestern.edu/index.php/Trust-region_methods](https://optimization.mccormick.northwestern.edu/index.php/Trust-region_methods)\n",
    "* [https://en.wikipedia.org/wiki/Trust_region](https://en.wikipedia.org/wiki/Trust_region)\n",
    "* [https://handwiki.org/wiki/COBYLA](https://handwiki.org/wiki/COBYLA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
